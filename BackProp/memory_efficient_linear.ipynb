{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### This showcases a way for implementing memory efficient linear layer\n",
        "\n",
        "SOURCE: https://medium.com/@yash9439/unslothais-innovative-hiring-challenge-memory-efficient-backprop-a5dc2372d469"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "ZuKiHQ1uPwHM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torch.nn import CrossEntropyLoss\n",
        "import time\n",
        "\n",
        "def transformation_function(batch, linear, labels):\n",
        "    x = linear(batch).float()\n",
        "    down_projection_function = CrossEntropyLoss(reduction=\"mean\")\n",
        "    loss = down_projection_function(x.view(-1, x.shape[-1]), labels.view(-1))\n",
        "    return loss\n",
        "\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, linear, labels, forward_function):\n",
        "        num_chunks = 4\n",
        "        chunks = X.chunk(num_chunks, dim=0)\n",
        "        if labels.dim() == 2:\n",
        "            split_sizes = [chunk.size(0) * X.size(1) for chunk in chunks]\n",
        "            labels_flat = labels.view(-1)\n",
        "            labels_chunks = torch.split(labels_flat, split_sizes, dim=0)\n",
        "        else:\n",
        "            split_sizes = None\n",
        "            labels_chunks = labels.chunk(num_chunks, dim=0)\n",
        "\n",
        "        sum_loss = 0.0\n",
        "        total_elements = 0\n",
        "        elements_in_chunks = []\n",
        "        for x_chunk, labels_chunk in zip(chunks, labels_chunks):\n",
        "            x_chunk = x_chunk.detach().requires_grad_(True)\n",
        "            chunk_loss = forward_function(x_chunk, linear, labels_chunk)\n",
        "            elements_in_chunk = labels_chunk.numel()\n",
        "            sum_loss += chunk_loss * elements_in_chunk\n",
        "            total_elements += elements_in_chunk\n",
        "            elements_in_chunks.append(elements_in_chunk)\n",
        "\n",
        "        final_loss = sum_loss / total_elements\n",
        "        ctx.save_for_backward(X, labels)\n",
        "        ctx.linear = linear\n",
        "        ctx.forward_function = forward_function\n",
        "        ctx.labels_dim = labels.dim()\n",
        "        ctx.split_sizes = split_sizes  # valid only if labels.dim() == 2\n",
        "        ctx.elements_in_chunks = elements_in_chunks\n",
        "        ctx.total_elements = total_elements\n",
        "        ctx.num_chunks = num_chunks\n",
        "        return final_loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dY):\n",
        "        X, labels = ctx.saved_tensors\n",
        "        linear = ctx.linear\n",
        "        forward_function = ctx.forward_function\n",
        "        num_chunks = ctx.num_chunks\n",
        "        elements_in_chunks = ctx.elements_in_chunks\n",
        "        total_elements = ctx.total_elements\n",
        "        X_chunks = X.chunk(num_chunks, dim=0)\n",
        "        if ctx.labels_dim == 2:\n",
        "            split_sizes = ctx.split_sizes\n",
        "            labels_flat = labels.view(-1)\n",
        "            labels_chunks = torch.split(labels_flat, split_sizes, dim=0)\n",
        "        else:\n",
        "            labels_chunks = labels.chunk(num_chunks, dim=0)\n",
        "\n",
        "        dX = torch.zeros_like(X)\n",
        "        dW = torch.zeros_like(linear.weight)\n",
        "        dB = torch.zeros_like(linear.bias) if linear.bias is not None else None\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            x_chunk = X_chunks[i].clone().detach().requires_grad_(True)\n",
        "            labels_chunk = labels_chunks[i]\n",
        "            elements_in_chunk = elements_in_chunks[i]\n",
        "            scale = dY * (elements_in_chunk / total_elements)\n",
        "            with torch.enable_grad():\n",
        "                chunk_loss = forward_function(x_chunk, linear, labels_chunk)\n",
        "                grad_x, grad_w, grad_b = torch.autograd.grad(\n",
        "                    chunk_loss,\n",
        "                    (x_chunk, linear.weight, linear.bias),\n",
        "                    grad_outputs=scale,\n",
        "                    retain_graph=False,\n",
        "                    allow_unused=False,\n",
        "                )\n",
        "            start_idx = i * x_chunk.size(0)\n",
        "            end_idx = start_idx + x_chunk.size(0)\n",
        "            dX[start_idx:end_idx] = grad_x\n",
        "            dW += grad_w\n",
        "            if grad_b is not None:\n",
        "                dB += grad_b\n",
        "        return dX, None, None, None\n",
        "\n",
        "# ---------------- Setup for CrossEntropy Test ----------------\n",
        "batch_size = 64\n",
        "seq_len = 512\n",
        "hidden_dim = 768\n",
        "vocab_size = 16000\n",
        "device = 'cuda'\n",
        "\n",
        "# Create input and labels with requires_grad=True for input_data\n",
        "input_data = torch.randn(batch_size, seq_len, hidden_dim,\n",
        "                         dtype=torch.bfloat16, device=device, requires_grad=True)\n",
        "labels = torch.randint(0, vocab_size, (batch_size, seq_len),\n",
        "                       dtype=torch.long, device=device)\n",
        "\n",
        "def create_linear():\n",
        "    linear = torch.nn.Linear(hidden_dim, vocab_size, device=device)\n",
        "    linear.weight = torch.nn.Parameter(linear.weight.to(torch.bfloat16))\n",
        "    if linear.bias is not None:\n",
        "        linear.bias.data = linear.bias.data.to(torch.bfloat16)\n",
        "    return linear"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1ROYqz6hoqi",
        "outputId": "b35447da-07e8-496e-ae4b-c5f48cec898e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Standard Loss (CE): 9.875000\n",
            "Custom Loss (CE): 9.850885\n",
            "CE Loss Difference: 0.024115\n",
            "\n",
            "Standard Time (CE): 1.023671 seconds\n",
            "Custom Time (CE): 1.390536 seconds\n",
            "Standard VRAM (CE): 4.063050 GiB\n",
            "Custom VRAM (CE): 1.760712 GiB\n",
            "\n",
            "Input gradient comparison (CE):\n",
            "Max difference between standard and custom input gradients: 7.450580596923828e-09\n"
          ]
        }
      ],
      "source": [
        "linear_standard = create_linear()\n",
        "linear_custom = create_linear()\n",
        "linear_custom.load_state_dict(linear_standard.state_dict())\n",
        "\n",
        "def standard_forward_backward():\n",
        "    linear_standard.zero_grad()\n",
        "    if input_data.grad is not None:\n",
        "        input_data.grad.zero_()\n",
        "    logits = linear_standard(input_data)\n",
        "    loss = CrossEntropyLoss(reduction='mean')(logits.view(-1, vocab_size), labels.view(-1))\n",
        "    loss.backward()\n",
        "    return loss.detach()\n",
        "\n",
        "def custom_forward_backward():\n",
        "    linear_custom.zero_grad()\n",
        "    if input_data.grad is not None:\n",
        "        input_data.grad.zero_()\n",
        "    loss = MemoryEfficientLinear.apply(input_data, linear_custom, labels, transformation_function)\n",
        "    loss.backward()\n",
        "    return loss.detach()\n",
        "\n",
        "# Warmup to avoid CUDA initialization overhead\n",
        "for _ in range(2):\n",
        "    standard_forward_backward()\n",
        "    custom_forward_backward()\n",
        "\n",
        "# ---------------- CrossEntropy Timing and VRAM Measurements ----------------\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.synchronize()\n",
        "start_time = time.time()\n",
        "loss_standard = standard_forward_backward()\n",
        "torch.cuda.synchronize()\n",
        "time_standard = time.time() - start_time\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "torch.cuda.synchronize()\n",
        "start_time = time.time()\n",
        "loss_custom = custom_forward_backward()\n",
        "torch.cuda.synchronize()\n",
        "time_custom = time.time() - start_time\n",
        "\n",
        "print(f\"Standard Loss (CE): {loss_standard.item():.6f}\")\n",
        "print(f\"Custom Loss (CE): {loss_custom.item():.6f}\")\n",
        "loss_diff = torch.abs(loss_standard - loss_custom).item()\n",
        "print(f\"CE Loss Difference: {loss_diff:.6f}\")\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "standard_forward_backward()\n",
        "mem_standard = torch.cuda.max_memory_allocated()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "custom_forward_backward()\n",
        "mem_custom = torch.cuda.max_memory_allocated()\n",
        "\n",
        "print(f\"\\nStandard Time (CE): {time_standard:.6f} seconds\")\n",
        "print(f\"Custom Time (CE): {time_custom:.6f} seconds\")\n",
        "print(f\"Standard VRAM (CE): {mem_standard / (1024**3):.6f} GiB\")\n",
        "print(f\"Custom VRAM (CE): {mem_custom / (1024**3):.6f} GiB\")\n",
        "\n",
        "# ---------------- CrossEntropy Input Gradients Comparison ----------------\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "if input_data.grad is not None:\n",
        "    input_data.grad.zero_()\n",
        "loss_standard = standard_forward_backward()\n",
        "grad_standard = input_data.grad.clone()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "if input_data.grad is not None:\n",
        "    input_data.grad.zero_()\n",
        "loss_custom = custom_forward_backward()\n",
        "grad_custom = input_data.grad.clone()\n",
        "\n",
        "print(\"\\nInput gradient comparison (CE):\")\n",
        "max_grad_diff = torch.max(torch.abs(grad_standard - grad_custom)).item()\n",
        "print(\"Max difference between standard and custom input gradients:\", max_grad_diff)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
