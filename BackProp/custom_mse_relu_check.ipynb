{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Testing with MSE & ReLU"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZuKiHQ1uPwHM"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "class MemoryEfficientLinear(torch.autograd.Function):\n",
        "    @staticmethod\n",
        "    def forward(ctx, X, linear, labels, forward_function):\n",
        "        num_chunks = 4\n",
        "        chunks = X.chunk(num_chunks, dim=0)\n",
        "        if labels.dim() == 2:\n",
        "            split_sizes = [chunk.size(0) * X.size(1) for chunk in chunks]\n",
        "            labels_flat = labels.view(-1)\n",
        "            labels_chunks = torch.split(labels_flat, split_sizes, dim=0)\n",
        "        else:\n",
        "            split_sizes = None\n",
        "            labels_chunks = labels.chunk(num_chunks, dim=0)\n",
        "\n",
        "        sum_loss = 0.0\n",
        "        total_elements = 0\n",
        "        elements_in_chunks = []\n",
        "        for x_chunk, labels_chunk in zip(chunks, labels_chunks):\n",
        "            x_chunk = x_chunk.detach().requires_grad_(True)\n",
        "            chunk_loss = forward_function(x_chunk, linear, labels_chunk)\n",
        "            elements_in_chunk = labels_chunk.numel()\n",
        "            sum_loss += chunk_loss * elements_in_chunk\n",
        "            total_elements += elements_in_chunk\n",
        "            elements_in_chunks.append(elements_in_chunk)\n",
        "\n",
        "        final_loss = sum_loss / total_elements\n",
        "        ctx.save_for_backward(X, labels)\n",
        "        ctx.linear = linear\n",
        "        ctx.forward_function = forward_function\n",
        "        ctx.labels_dim = labels.dim()\n",
        "        ctx.split_sizes = split_sizes  # valid only if labels.dim() == 2\n",
        "        ctx.elements_in_chunks = elements_in_chunks\n",
        "        ctx.total_elements = total_elements\n",
        "        ctx.num_chunks = num_chunks\n",
        "        return final_loss\n",
        "\n",
        "    @staticmethod\n",
        "    def backward(ctx, dY):\n",
        "        X, labels = ctx.saved_tensors\n",
        "        linear = ctx.linear\n",
        "        forward_function = ctx.forward_function\n",
        "        num_chunks = ctx.num_chunks\n",
        "        elements_in_chunks = ctx.elements_in_chunks\n",
        "        total_elements = ctx.total_elements\n",
        "        X_chunks = X.chunk(num_chunks, dim=0)\n",
        "        if ctx.labels_dim == 2:\n",
        "            split_sizes = ctx.split_sizes\n",
        "            labels_flat = labels.view(-1)\n",
        "            labels_chunks = torch.split(labels_flat, split_sizes, dim=0)\n",
        "        else:\n",
        "            labels_chunks = labels.chunk(num_chunks, dim=0)\n",
        "\n",
        "        dX = torch.zeros_like(X)\n",
        "        dW = torch.zeros_like(linear.weight)\n",
        "        dB = torch.zeros_like(linear.bias) if linear.bias is not None else None\n",
        "\n",
        "        for i in range(num_chunks):\n",
        "            x_chunk = X_chunks[i].clone().detach().requires_grad_(True)\n",
        "            labels_chunk = labels_chunks[i]\n",
        "            elements_in_chunk = elements_in_chunks[i]\n",
        "            scale = dY * (elements_in_chunk / total_elements)\n",
        "            with torch.enable_grad():\n",
        "                chunk_loss = forward_function(x_chunk, linear, labels_chunk)\n",
        "                grad_x, grad_w, grad_b = torch.autograd.grad(\n",
        "                    chunk_loss,\n",
        "                    (x_chunk, linear.weight, linear.bias),\n",
        "                    grad_outputs=scale,\n",
        "                    retain_graph=False,\n",
        "                    allow_unused=False,\n",
        "                )\n",
        "            start_idx = i * x_chunk.size(0)\n",
        "            end_idx = start_idx + x_chunk.size(0)\n",
        "            dX[start_idx:end_idx] = grad_x\n",
        "            dW += grad_w\n",
        "            if grad_b is not None:\n",
        "                dB += grad_b\n",
        "        return dX, None, None, None\n",
        "\n",
        "# ---------------- Setup for CrossEntropy Test ----------------\n",
        "batch_size = 64\n",
        "seq_len = 512\n",
        "hidden_dim = 768\n",
        "vocab_size = 16000\n",
        "device = 'cuda'\n",
        "\n",
        "# Create input and labels with requires_grad=True for input_data\n",
        "input_data = torch.randn(batch_size, seq_len, hidden_dim,\n",
        "                         dtype=torch.bfloat16, device=device, requires_grad=True)\n",
        "labels = torch.randint(0, vocab_size, (batch_size, seq_len),\n",
        "                       dtype=torch.long, device=device)\n",
        "\n",
        "def create_linear():\n",
        "    linear = torch.nn.Linear(hidden_dim, vocab_size, device=device)\n",
        "    linear.weight = torch.nn.Parameter(linear.weight.to(torch.bfloat16))\n",
        "    if linear.bias is not None:\n",
        "        linear.bias.data = linear.bias.data.to(torch.bfloat16)\n",
        "    return linear"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1ROYqz6hoqi",
        "outputId": "e56cd08d-3d57-4c9e-880b-0267f6c3fc27"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "MSE Loss Test (with ReLU activation):\n",
            "Standard MSE Loss: 1.166796\n",
            "Custom MSE Loss: 1.333750\n",
            "MSE Loss Difference: 0.166953\n"
          ]
        }
      ],
      "source": [
        "targets_mse = torch.randn(batch_size, seq_len, vocab_size, dtype=torch.float32, device=device)\n",
        "\n",
        "def transformation_function_mse(batch, linear, targets):\n",
        "    x = linear(batch).float()\n",
        "    # x = torch.relu(x)  # Activation\n",
        "    mse_loss_fn = torch.nn.MSELoss(reduction=\"mean\")\n",
        "    loss = mse_loss_fn(x, targets)\n",
        "    return loss\n",
        "\n",
        "linear_standard_mse = create_linear()\n",
        "linear_custom_mse = create_linear()\n",
        "linear_custom_mse.load_state_dict(linear_standard_mse.state_dict())\n",
        "\n",
        "def standard_forward_backward_mse():\n",
        "    linear_standard_mse.zero_grad()\n",
        "    if input_data.grad is not None:\n",
        "        input_data.grad.zero_()\n",
        "    logits = linear_standard_mse(input_data)\n",
        "    activated = torch.relu(logits.float())\n",
        "    loss = torch.nn.MSELoss(reduction='mean')(activated, targets_mse)\n",
        "    loss.backward()\n",
        "    return loss.detach()\n",
        "\n",
        "def custom_forward_backward_mse():\n",
        "    linear_custom_mse.zero_grad()\n",
        "    if input_data.grad is not None:\n",
        "        input_data.grad.zero_()\n",
        "    loss = MemoryEfficientLinear.apply(input_data, linear_custom_mse, targets_mse, transformation_function_mse)\n",
        "    loss.backward()\n",
        "    return loss.detach()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "loss_standard_mse = standard_forward_backward_mse()\n",
        "\n",
        "torch.cuda.empty_cache()\n",
        "torch.cuda.reset_peak_memory_stats()\n",
        "loss_custom_mse = custom_forward_backward_mse()\n",
        "loss_diff_mse = torch.abs(loss_standard_mse - loss_custom_mse).item()\n",
        "\n",
        "print(\"\\nMSE Loss Test (with ReLU activation):\")\n",
        "print(f\"Standard MSE Loss: {loss_standard_mse.item():.6f}\")\n",
        "print(f\"Custom MSE Loss: {loss_custom_mse.item():.6f}\")\n",
        "print(f\"MSE Loss Difference: {loss_diff_mse:.6f}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
