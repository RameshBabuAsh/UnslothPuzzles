{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "**Link to Colab :** [NF4 - Triton](https://colab.research.google.com/drive/1zp8zvbRl1V3_WKwSlbWnf5I6p30GdFkJ?usp=sharing)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kYT9mXtdfWSV"
      },
      "source": [
        "### Installing Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "TwoKw8eguULB"
      },
      "outputs": [],
      "source": [
        "# Code to install Unsloth, Triton, Torch etc\n",
        "%%capture\n",
        "!pip install --no-deps bitsandbytes accelerate xformers==0.0.29 peft trl triton\n",
        "!pip install --no-deps cut_cross_entropy unsloth_zoo\n",
        "!pip install sentencepiece protobuf datasets huggingface_hub hf_transfer\n",
        "!pip install --no-deps unsloth"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XhBqnf4ufv8l"
      },
      "source": [
        "### Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PbwJz8ZeIQBt",
        "outputId": "57b354e6-c84a-4092-bdbb-18aead90dc05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ðŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n",
            "ðŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n"
          ]
        }
      ],
      "source": [
        "# Helpful functions used through the entire notebook\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import unsloth\n",
        "from transformers import set_seed\n",
        "import time\n",
        "import inspect\n",
        "import os\n",
        "from bitsandbytes.nn import Linear4bit\n",
        "from unsloth.kernels.utils import fast_dequantize\n",
        "from transformers.activations import ACT2FN\n",
        "from peft.utils.integrations import dequantize_module_weight as peft_dequantize\n",
        "major_version, minor_version = torch.cuda.get_device_capability()\n",
        "HAS_BFLOAT16 = (major_version >= 8)\n",
        "from inspect import currentframe as _C, getframeinfo\n",
        "_F = lambda c: getframeinfo(c).lineno # Gets line number\n",
        "WARN = lambda x: print(f\"\\033[31m{x}\\033[0m\") # Red colored warnings\n",
        "\n",
        "# https://stackoverflow.com/questions/18425225/getting-the-name-of-a-variable-as-a-string\n",
        "def NAME(var):\n",
        "    callers_local_vars = inspect.currentframe().f_back.f_locals.items()\n",
        "    names = [var_name for var_name, var_val in callers_local_vars if var_val is var]\n",
        "    return names[0] if len(names) != 0 else \"\"\n",
        "\n",
        "def assert_same(x, y, line, dtype):\n",
        "    assert(x.dtype == dtype)\n",
        "    try: torch.testing.assert_close(x, y, check_stride = True, atol=0.01, rtol=0.1)\n",
        "    except Exception as error:\n",
        "        raise RuntimeError(\n",
        "            f\"Failed allclose at line [{line}]: {NAME(x)}, {NAME(y)}\\n{str(error)}\"\n",
        "        )\n",
        "\n",
        "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1nxSa2dmgJd0"
      },
      "source": [
        "### Initial Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "fnpQPR4oIT_9"
      },
      "outputs": [],
      "source": [
        "def unsloth_dequantize(weight):\n",
        "    return fast_dequantize(weight.weight, weight.weight.quant_state)\n",
        "\n",
        "def bnb_Linear4bit(hd, m, dtype = torch.float16):\n",
        "    return Linear4bit(\n",
        "        hd, m, bias = None,\n",
        "        compute_dtype       = dtype,\n",
        "        compress_statistics = True,\n",
        "        quant_type          = \"nf4\",\n",
        "    )\n",
        "\n",
        "# [NEW] as at 18th Feb 2025\n",
        "def assert_correct_bnb(weight, dtype):\n",
        "    assert(weight.weight.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.dtype == dtype)\n",
        "    assert(weight.weight.quant_state.absmax.dtype == torch.uint8)\n",
        "    assert(weight.weight.quant_state.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.offset.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.blocksize == 64)\n",
        "    assert(weight.weight.quant_state.state2.absmax.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.code.dtype == torch.float32)\n",
        "    assert(weight.weight.quant_state.state2.blocksize == 256)\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, hd = 4096, m = 14336, dtype = torch.float16):\n",
        "        super().__init__()\n",
        "        self.gate_proj = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.up_proj   = bnb_Linear4bit(hd, m, dtype = dtype).to(\"cuda\")\n",
        "        self.down_proj = bnb_Linear4bit(m, hd, dtype = dtype).to(\"cuda\")\n",
        "        # [NEW] as at 18th Feb 2025\n",
        "        self.gate_proj.weight.quant_state.dtype = dtype\n",
        "        self.up_proj  .weight.quant_state.dtype = dtype\n",
        "        self.down_proj.weight.quant_state.dtype = dtype\n",
        "        self.act_fn = ACT2FN[\"silu\"]\n",
        "    def forward(self, x):\n",
        "        return self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
        "\n",
        "def mlp_forward(X, mlp, fx):\n",
        "    up   = X @ fx(mlp.  up_proj).t()\n",
        "    gate = X @ fx(mlp.gate_proj).t()\n",
        "    h = mlp.act_fn(gate) * up\n",
        "    down = h @ fx(mlp.down_proj).t()\n",
        "    return down\n",
        "\n",
        "def mlp_dequantize(X, mlp, fx):\n",
        "    a = fx(mlp.  up_proj).t(); torch.cuda.synchronize()\n",
        "    b = fx(mlp.gate_proj).t(); torch.cuda.synchronize()\n",
        "    c = fx(mlp.down_proj).t(); torch.cuda.synchronize()\n",
        "    return a, b, c\n",
        "\n",
        "def test_dequantize(dequantize_fx):\n",
        "    elapsed = 0\n",
        "    options = [\n",
        "        (2, 3333, 2048,  8192, 3407, torch.float16),\n",
        "        (5,  777, 1024,  4096, 3409, torch.float16),\n",
        "        (3, 2048, 4096, 14336, 3408, torch.float16),\n",
        "    ]\n",
        "    for (bsz, qlen, hd, m, seed, dt) in options:\n",
        "        set_seed(seed)\n",
        "        torch.set_default_dtype(torch.float32)\n",
        "        mlp = MLP(hd = hd, m = m, dtype = dt)\n",
        "        X = torch.randn((bsz, qlen, hd), device = \"cuda\", dtype = dt)\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "        # Warmup\n",
        "        for _ in range(2):\n",
        "            assert_same( mlp_forward(X, mlp, unsloth_dequantize), mlp(X), _F(_C()), dt)\n",
        "            assert_same( mlp_forward(X, mlp, dequantize_fx), mlp(X), _F(_C()), dt)\n",
        "            # [NEW] as at 18th Feb 2025\n",
        "            assert_correct_bnb(mlp.  up_proj, dt)\n",
        "            assert_correct_bnb(mlp.gate_proj, dt)\n",
        "            assert_correct_bnb(mlp.down_proj, dt)\n",
        "            a, b, c = mlp_dequantize(X, mlp, dequantize_fx)\n",
        "            A, B, C = mlp_dequantize(X, mlp, unsloth_dequantize)\n",
        "            assert_same(a, A, _F(_C()), dt)\n",
        "            assert_same(b, B, _F(_C()), dt)\n",
        "            assert_same(c, C, _F(_C()), dt)\n",
        "\n",
        "        # Benchmarking\n",
        "        torch.cuda.synchronize()\n",
        "        start = time.time()\n",
        "        for _ in range(1000): mlp_dequantize(X, mlp, dequantize_fx)\n",
        "        elapsed += time.time() - start\n",
        "    return elapsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1o7Ex5e0gPOu"
      },
      "source": [
        "### Unsloth Dequantize Speed Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fVU75CM_QtWs",
        "outputId": "7b9d1d57-7b1d-4d69-f8f2-967646448d87"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "6.7735817432403564"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dequantize(unsloth_dequantize)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiDtdWGpgfch"
      },
      "source": [
        "### Custom Triton Code"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "iAgHwzPoIYmm"
      },
      "outputs": [],
      "source": [
        "from triton import jit\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def lookup_const(x):\n",
        "    result = tl.where(x == 0, -1.0, 0.0)\n",
        "    result = tl.where(x == 1, -0.6961928009986877, result)\n",
        "    result = tl.where(x == 2, -0.5250730514526367, result)\n",
        "    result = tl.where(x == 3, -0.39491748809814453, result)\n",
        "    result = tl.where(x == 4, -0.28444138169288635, result)\n",
        "    result = tl.where(x == 5, -0.18477343022823334, result)\n",
        "    result = tl.where(x == 6, -0.09105003625154495, result)\n",
        "    result = tl.where(x == 7,  0.0, result)\n",
        "    result = tl.where(x == 8,  0.07958029955625534, result)\n",
        "    result = tl.where(x == 9,  0.16093020141124725, result)\n",
        "    result = tl.where(x == 10, 0.24611230194568634, result)\n",
        "    result = tl.where(x == 11, 0.33791524171829224, result)\n",
        "    result = tl.where(x == 12, 0.44070982933044434, result)\n",
        "    result = tl.where(x == 13, 0.5626170039176941, result)\n",
        "    result = tl.where(x == 14, 0.7229568362236023, result)\n",
        "    result = tl.where(x == 15, 1.0, result)\n",
        "    return result\n",
        "\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel(weight_ptr, out_ptr, absmax_ptr, absmax2_ptr, code2_ptr, residue, n_elements, BLOCK_SIZE: tl.constexpr):\n",
        "\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    out_offsets = 2 * pid * BLOCK_SIZE + tl.arange(0, 2*BLOCK_SIZE)\n",
        "    mask = offsets < n_elements\n",
        "    out_mask = out_offsets < 2 * n_elements\n",
        "\n",
        "    weight_val = tl.load(weight_ptr + offsets, mask=mask)\n",
        "\n",
        "    high_nibble = weight_val >> 4\n",
        "    low_nibble  = weight_val & 0x0F\n",
        "\n",
        "    high_nibble_f32  = lookup_const(high_nibble)\n",
        "    low_nibble_f32  = lookup_const(low_nibble)\n",
        "\n",
        "    current_absmax = tl.load(absmax_ptr + (offsets >> 5), mask = mask)\n",
        "    decoded_absmax = tl.load(code2_ptr + current_absmax, mask = mask)\n",
        "\n",
        "    current_absmax2 = tl.load(absmax2_ptr + (offsets >> 13), mask=mask)\n",
        "\n",
        "    scaling = tl.fma(decoded_absmax, current_absmax2, residue)\n",
        "\n",
        "    high_val = high_nibble_f32 * scaling\n",
        "    low_val = low_nibble_f32 * scaling\n",
        "\n",
        "    out = tl.interleave(high_val, low_val)\n",
        "\n",
        "    tl.store(out_ptr + out_offsets,  out, mask=out_mask)\n",
        "\n",
        "def _your_dequantize_nf4(weight, quant_state, offset):\n",
        "\n",
        "    n_elements = weight.numel()\n",
        "    out = torch.empty(2*n_elements, device=weight.device, dtype=quant_state.dtype)\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "    grid = lambda meta: ((n_elements + meta['BLOCK_SIZE'] - 1) // meta['BLOCK_SIZE'],)\n",
        "\n",
        "    _your_dequantize_nf4_kernel[grid](\n",
        "        weight,\n",
        "        out,\n",
        "        quant_state.absmax,\n",
        "        quant_state.state2.absmax,\n",
        "        quant_state.state2.code,\n",
        "        quant_state.offset.item(),\n",
        "        n_elements,\n",
        "        BLOCK_SIZE\n",
        "    )\n",
        "    return out.view(quant_state.shape)\n",
        "\n",
        "def your_dequantize_nf4(weight):\n",
        "    offset = weight.weight.quant_state.offset\n",
        "    return _your_dequantize_nf4(weight.weight.data, weight.weight.quant_state, offset)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgdPn2XtglHL"
      },
      "source": [
        "### Custom Code Speed Test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ycW0xJMXLFM8",
        "outputId": "72e5e6d5-e73c-407d-dba2-6ff6928b79a0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "4.116127014160156"
            ]
          },
          "execution_count": 6,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_dequantize(your_dequantize_nf4)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vj1pWE0ngtjx",
        "outputId": "e942a9f7-81a7-4b9a-9876-722f1f4da56b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1.1454133907231538\n",
            "1.1647778856211157\n",
            "1.417303095420293\n",
            "1.1747517897251316\n",
            "1.1881643938005837\n",
            "Ratio Of Speed between Unsloth & Custom Code Is 1.2180821110580555\n"
          ]
        }
      ],
      "source": [
        "time_taken = 0\n",
        "sample_runs = 5\n",
        "for sample_run in range(sample_runs):\n",
        "    val = (test_dequantize(unsloth_dequantize)/test_dequantize(your_dequantize_nf4))\n",
        "    print(val)\n",
        "    time_taken += val\n",
        "print(\"Ratio Of Speed between Unsloth & Custom Code Is\", time_taken/sample_runs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NidVKEyGiJan"
      },
      "source": [
        "### Compilation Check With `torch.compile`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "L6ThCGLQziSL",
        "outputId": "b176ebe6-9d57-46c3-89a7-802239a226d4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.compile works with triton code -- test passed\n"
          ]
        }
      ],
      "source": [
        "from triton import jit\n",
        "import triton\n",
        "import triton.language as tl\n",
        "\n",
        "@triton.jit\n",
        "def lookup_const_compiled(x):\n",
        "    result = tl.where(x == 0, -1.0, 0.0)\n",
        "    result = tl.where(x == 1, -0.6961928009986877, result)\n",
        "    result = tl.where(x == 2, -0.5250730514526367, result)\n",
        "    result = tl.where(x == 3, -0.39491748809814453, result)\n",
        "    result = tl.where(x == 4, -0.28444138169288635, result)\n",
        "    result = tl.where(x == 5, -0.18477343022823334, result)\n",
        "    result = tl.where(x == 6, -0.09105003625154495, result)\n",
        "    result = tl.where(x == 7,  0.0, result)\n",
        "    result = tl.where(x == 8,  0.07958029955625534, result)\n",
        "    result = tl.where(x == 9,  0.16093020141124725, result)\n",
        "    result = tl.where(x == 10, 0.24611230194568634, result)\n",
        "    result = tl.where(x == 11, 0.33791524171829224, result)\n",
        "    result = tl.where(x == 12, 0.44070982933044434, result)\n",
        "    result = tl.where(x == 13, 0.5626170039176941, result)\n",
        "    result = tl.where(x == 14, 0.7229568362236023, result)\n",
        "    result = tl.where(x == 15, 1.0, result)\n",
        "    return result\n",
        "\n",
        "@triton.jit\n",
        "def _your_dequantize_nf4_kernel_compiled(weight_ptr, out_ptr, absmax_ptr, absmax2_ptr, code2_ptr, residue, n_elements, BLOCK_SIZE: tl.constexpr):\n",
        "\n",
        "    pid = tl.program_id(0)\n",
        "    offsets = pid * BLOCK_SIZE + tl.arange(0, BLOCK_SIZE)\n",
        "    out_offsets = 2 * pid * BLOCK_SIZE + tl.arange(0, 2*BLOCK_SIZE)\n",
        "    mask = offsets < n_elements\n",
        "    out_mask = out_offsets < 2 * n_elements\n",
        "\n",
        "    weight_val = tl.load(weight_ptr + offsets, mask=mask)\n",
        "\n",
        "    high_nibble = weight_val >> 4\n",
        "    low_nibble  = weight_val & 0x0F\n",
        "\n",
        "    high_nibble_f32  = lookup_const_compiled(high_nibble)\n",
        "    low_nibble_f32  = lookup_const_compiled(low_nibble)\n",
        "\n",
        "    current_absmax = tl.load(absmax_ptr + (offsets >> 5), mask = mask)\n",
        "    decoded_absmax = tl.load(code2_ptr + current_absmax, mask = mask)\n",
        "\n",
        "    current_absmax2 = tl.load(absmax2_ptr + (offsets >> 13), mask=mask)\n",
        "\n",
        "    scaling = tl.fma(decoded_absmax, current_absmax2, residue)\n",
        "\n",
        "    high_val = high_nibble_f32 * scaling\n",
        "    low_val = low_nibble_f32 * scaling\n",
        "\n",
        "    out = tl.interleave(high_val, low_val)\n",
        "\n",
        "    tl.store(out_ptr + out_offsets,  out, mask=out_mask)\n",
        "\n",
        "torch._dynamo.config.capture_scalar_outputs = True\n",
        "\n",
        "torch_compile_options = torch_compile_options = {\n",
        "    \"epilogue_fusion\"   : True,\n",
        "    \"max_autotune\"      : True,\n",
        "    \"shape_padding\"     : True,\n",
        "    \"trace.enabled\"     : True,\n",
        "    \"triton.cudagraphs\" : False,\n",
        "}\n",
        "\n",
        "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
        "def _your_dequantize_nf4_compiled(weight, quant_state, offset):\n",
        "\n",
        "    n_elements = weight.numel()\n",
        "    out = torch.empty(2*n_elements, device=weight.device, dtype=quant_state.dtype)\n",
        "\n",
        "    BLOCK_SIZE = 1024\n",
        "    grid = lambda meta: ((n_elements + meta['BLOCK_SIZE'] - 1) // meta['BLOCK_SIZE'],)\n",
        "\n",
        "    _your_dequantize_nf4_kernel_compiled[grid](\n",
        "        weight,\n",
        "        out,\n",
        "        quant_state.absmax,\n",
        "        quant_state.state2.absmax,\n",
        "        quant_state.state2.code,\n",
        "        offset,\n",
        "        n_elements,\n",
        "        BLOCK_SIZE\n",
        "    )\n",
        "    return out.view(quant_state.shape)\n",
        "\n",
        "def your_dequantize_nf4_compiled(weight):\n",
        "    offset = weight.weight.quant_state.offset.item()\n",
        "    return _your_dequantize_nf4_compiled(weight.weight.data, weight.weight.quant_state, offset)\n",
        "\n",
        "try:\n",
        "    test_dequantize(your_dequantize_nf4_compiled)\n",
        "    print(\"torch.compile works with triton code -- test passed\")\n",
        "except:\n",
        "    print(\"torch.compile does not work with triton code -- test failed\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
