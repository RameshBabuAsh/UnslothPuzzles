{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "from typing import Optional, Tuple\n",
    "from datasets import load_dataset\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "import torch\n",
    "import transformers.models.llama.modeling_llama\n",
    "from transformers.models.llama.modeling_llama import Cache, Unpack, FlashAttentionKwargs, Callable, eager_attention_forward, apply_rotary_pos_emb, ALL_ATTENTION_FUNCTIONS, logger, BaseModelOutputWithPast, Union, DynamicCache\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from transformers.modeling_utils import PreTrainedModel\n",
    "from peft import get_peft_model, LoraConfig, TaskType"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuring Environment Variables and PyTorch Settings\n",
    "\n",
    "This script sets environment variables for optimizing PyTorch execution and debugging. It enables verbose logging, configures CUDA settings, and sets TorchInductor and TorchDynamo options for efficient model compilation.\n",
    "\n",
    "### **Torch Compile Options:**\n",
    "Defines optimization settings like epilogue fusion, autotuning, and shape padding.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"TORCHDYNAMO_VERBOSE\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_FORCE_DISABLE_CACHES\"] = \"1\"\n",
    "os.environ[\"TORCHINDUCTOR_COMPILE_THREADS\"] = \"1\"\n",
    "os.environ[\"HF_HUB_ENABLE_HF_TRANSFER\"] = \"1\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0,1\"\n",
    "os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \\\n",
    "    \"expandable_segments:True,\"\\\n",
    "    \"roundup_power2_divisions:[32:256,64:128,256:64,>:32]\"\n",
    "\n",
    "\n",
    "torch._inductor.config.debug = True\n",
    "torch._logging.set_logs(\n",
    "    dynamo = logging.WARN,\n",
    "    inductor = logging.WARN,\n",
    "    graph_breaks = True,\n",
    "    recompiles = True,\n",
    "    recompiles_verbose = True,\n",
    "    compiled_autograd_verbose = True,\n",
    "    # aot_joint_graph = True, # Enable for more logs\n",
    "    # aot_graphs = True,\n",
    ")\n",
    "torch._dynamo.config.verbose = True\n",
    "torch._dynamo.config.suppress_errors = False\n",
    "\n",
    "torch_compile_options = torch_compile_options = {\n",
    "    \"epilogue_fusion\"   : True,\n",
    "    \"max_autotune\"      : True,\n",
    "    \"shape_padding\"     : True,\n",
    "    \"trace.enabled\"     : True,\n",
    "    \"triton.cudagraphs\" : False,\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Gradient Modification in PyTorch\n",
    "\n",
    "This script defines a custom PyTorch operation `gradChanger` to modify tensor gradients, ensuring detached clones retain `requires_grad=True`.  \n",
    "\n",
    "- **`@torch.library.custom_op`**: Registers a custom operation (`noName::gradChanger`).  \n",
    "- **`@gradChanger.register_fake`**: Defines a fake implementation for testing.  \n",
    "- **`modified_enable_input_require_grads`**: Hooks into the forward pass of `PreTrainedModel` to apply `gradChanger` to input embeddings.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.library.custom_op(\"noName::gradChanger\", mutates_args=())\n",
    "def gradChanger(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = x.detach().clone().requires_grad_()\n",
    "    return y\n",
    "\n",
    "@gradChanger.register_fake\n",
    "def _(x: torch.Tensor) -> torch.Tensor:\n",
    "    y = x.detach().clone().requires_grad_()\n",
    "    return y\n",
    "\n",
    "def modified_enable_input_require_grads(self):\n",
    "    def make_inputs_require_grads(module, input, output):\n",
    "        return gradChanger(output)\n",
    "    self._require_grads_hook = self.get_input_embeddings().register_forward_hook(make_inputs_require_grads)\n",
    "\n",
    "PreTrainedModel.enable_input_require_grads = modified_enable_input_require_grads"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compiling Almost Every LLaMA Model Components With `torch.compile`\n",
    "\n",
    "This script compiles different components of the LLaMA model using `torch.compile` for performance optimization.\n",
    "\n",
    "- **`compiled_llama_mlp`**: Optimized MLP forward pass.\n",
    "- **`compiled_llama_rms_norm`**: Efficient RMSNorm implementation.\n",
    "- **`compiled_llama_rotary_embedding`**: Computes rotary position embeddings.\n",
    "- **`compiled_llama_attention`**: Custom attention mechanism.\n",
    "- **`compiled_llama_decoder_layer`**: Processes each decoder layer.\n",
    "- **`compiled_llama_model`**: Full LLaMA forward pass with caching.\n",
    "- **`compiled_compute_loss`**: Computes cross-entropy loss for training.\n",
    "\n",
    "Each function is compiled with `torch.compile(fullgraph=True, dynamic=True, options=torch_compile_options)`, and replaces corresponding methods in `transformers.models.llama.modeling_llama`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_mlp(self, x):\n",
    "    down_proj = self.down_proj(self.act_fn(self.gate_proj(x)) * self.up_proj(x))\n",
    "    return down_proj\n",
    "\n",
    "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_rms_norm(self, hidden_states):\n",
    "    input_dtype = hidden_states.dtype\n",
    "    hidden_states = hidden_states.to(torch.float32)\n",
    "    variance = hidden_states.pow(2).mean(-1, keepdim=True)\n",
    "    hidden_states = hidden_states * torch.rsqrt(variance + self.variance_epsilon)\n",
    "    return self.weight * hidden_states.to(input_dtype)\n",
    "\n",
    "@torch.no_grad()\n",
    "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_rotary_embedding(self, x, position_ids):\n",
    "    if \"dynamic\" in self.rope_type:\n",
    "        self._dynamic_frequency_update(position_ids, device=x.device)\n",
    "    inv_freq_expanded = self.inv_freq[None, :, None].float().expand(position_ids.shape[0], -1, 1)\n",
    "    position_ids_expanded = position_ids[:, None, :].float()\n",
    "    device_type = x.device.type\n",
    "    device_type = device_type if isinstance(device_type, str) and device_type != \"mps\" else \"cpu\"\n",
    "    with torch.autocast(device_type=device_type, enabled=False):\n",
    "        freqs = (inv_freq_expanded.float() @ position_ids_expanded.float()).transpose(1, 2)\n",
    "        emb = torch.cat((freqs, freqs), dim=-1)\n",
    "        cos = emb.cos()\n",
    "        sin = emb.sin()\n",
    "    cos = cos * self.attention_scaling\n",
    "    sin = sin * self.attention_scaling\n",
    "    return cos.to(dtype=x.dtype), sin.to(dtype=x.dtype)\n",
    "\n",
    "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_attention(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        position_embeddings: Tuple[torch.Tensor, torch.Tensor],\n",
    "        attention_mask: Optional[torch.Tensor],\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.Tensor, Optional[torch.Tensor], Optional[Tuple[torch.Tensor]]]:\n",
    "        input_shape = hidden_states.shape[:-1]\n",
    "        hidden_shape = (*input_shape, -1, self.head_dim)\n",
    "\n",
    "        query_states = self.q_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        key_states = self.k_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "        value_states = self.v_proj(hidden_states).view(hidden_shape).transpose(1, 2)\n",
    "\n",
    "        cos, sin = position_embeddings\n",
    "        query_states, key_states = apply_rotary_pos_emb(query_states, key_states, cos, sin)\n",
    "\n",
    "        if past_key_value is not None:\n",
    "            # sin and cos are specific to RoPE models; cache_position needed for the static cache\n",
    "            cache_kwargs = {\"sin\": sin, \"cos\": cos, \"cache_position\": cache_position}\n",
    "            key_states, value_states = past_key_value.update(key_states, value_states, self.layer_idx, cache_kwargs)\n",
    "\n",
    "        attention_interface: Callable = eager_attention_forward\n",
    "        if self.config._attn_implementation != \"eager\":\n",
    "            if self.config._attn_implementation == \"sdpa\" and kwargs.get(\"output_attentions\", False):\n",
    "                logger.warning_once(\n",
    "                    \"`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to \"\n",
    "                    'eager attention. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.'\n",
    "                )\n",
    "            else:\n",
    "                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]\n",
    "\n",
    "        attn_output, attn_weights = attention_interface(\n",
    "            self,\n",
    "            query_states,\n",
    "            key_states,\n",
    "            value_states,\n",
    "            attention_mask,\n",
    "            dropout=0.0 if not self.training else self.attention_dropout,\n",
    "            scaling=self.scaling,\n",
    "            **kwargs,\n",
    "        )\n",
    "\n",
    "        attn_output = attn_output.reshape(*input_shape, -1).contiguous()\n",
    "        attn_output = self.o_proj(attn_output)\n",
    "        return attn_output, attn_weights\n",
    "\n",
    "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_decoder_layer(\n",
    "        self,\n",
    "        hidden_states: torch.Tensor,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_value: Optional[Cache] = None,\n",
    "        output_attentions: Optional[bool] = False,\n",
    "        use_cache: Optional[bool] = False,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        position_embeddings: Optional[Tuple[torch.Tensor, torch.Tensor]] = None,  # necessary, but kept here for BC\n",
    "        **kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Tuple[torch.FloatTensor, Optional[Tuple[torch.FloatTensor, torch.FloatTensor]]]:\n",
    "        residual = hidden_states\n",
    "\n",
    "        hidden_states = self.input_layernorm(hidden_states)\n",
    "\n",
    "        # Self Attention\n",
    "        hidden_states, self_attn_weights = self.self_attn(\n",
    "            hidden_states=hidden_states,\n",
    "            attention_mask=attention_mask,\n",
    "            position_ids=position_ids,\n",
    "            past_key_value=past_key_value,\n",
    "            output_attentions=output_attentions,\n",
    "            use_cache=use_cache,\n",
    "            cache_position=cache_position,\n",
    "            position_embeddings=position_embeddings,\n",
    "            **kwargs,\n",
    "        )\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        # Fully Connected\n",
    "        residual = hidden_states\n",
    "        hidden_states = self.post_attention_layernorm(hidden_states)\n",
    "        hidden_states = self.mlp(hidden_states)\n",
    "        hidden_states = residual + hidden_states\n",
    "\n",
    "        outputs = (hidden_states,)\n",
    "        if output_attentions:\n",
    "            outputs += (self_attn_weights,)\n",
    "\n",
    "        return outputs\n",
    "\n",
    "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
    "def compiled_llama_model(\n",
    "        self,\n",
    "        input_ids: torch.LongTensor = None,\n",
    "        attention_mask: Optional[torch.Tensor] = None,\n",
    "        position_ids: Optional[torch.LongTensor] = None,\n",
    "        past_key_values: Optional[Cache] = None,\n",
    "        inputs_embeds: Optional[torch.FloatTensor] = None,\n",
    "        use_cache: Optional[bool] = None,\n",
    "        output_attentions: Optional[bool] = None,\n",
    "        output_hidden_states: Optional[bool] = None,\n",
    "        return_dict: Optional[bool] = None,\n",
    "        cache_position: Optional[torch.LongTensor] = None,\n",
    "        **flash_attn_kwargs: Unpack[FlashAttentionKwargs],\n",
    "    ) -> Union[Tuple, BaseModelOutputWithPast]:\n",
    "        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions\n",
    "        output_hidden_states = (\n",
    "            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states\n",
    "        )\n",
    "        use_cache = use_cache if use_cache is not None else self.config.use_cache\n",
    "        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n",
    "\n",
    "        if (input_ids is None) ^ (inputs_embeds is not None):\n",
    "            raise ValueError(\"You must specify exactly one of input_ids or inputs_embeds\")\n",
    "\n",
    "        if self.gradient_checkpointing and self.training and use_cache:\n",
    "            logger.warning_once(\n",
    "                \"`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\"\n",
    "            )\n",
    "            use_cache = False\n",
    "\n",
    "        if inputs_embeds is None:\n",
    "            inputs_embeds = self.embed_tokens(input_ids)\n",
    "\n",
    "        if use_cache and past_key_values is None:\n",
    "            past_key_values = DynamicCache()\n",
    "\n",
    "        if cache_position is None:\n",
    "            past_seen_tokens = past_key_values.get_seq_length() if past_key_values is not None else 0\n",
    "            cache_position = torch.arange(\n",
    "                past_seen_tokens, past_seen_tokens + inputs_embeds.shape[1], device=inputs_embeds.device\n",
    "            )\n",
    "\n",
    "        if position_ids is None:\n",
    "            position_ids = cache_position.unsqueeze(0)\n",
    "\n",
    "        causal_mask = self._update_causal_mask(\n",
    "            attention_mask, inputs_embeds, cache_position, past_key_values, output_attentions\n",
    "        )\n",
    "\n",
    "        hidden_states = inputs_embeds\n",
    "\n",
    "        # create position embeddings to be shared across the decoder layers\n",
    "        position_embeddings = self.rotary_emb(hidden_states, position_ids)\n",
    "\n",
    "        # decoder layers\n",
    "        all_hidden_states = () if output_hidden_states else None\n",
    "        all_self_attns = () if output_attentions else None\n",
    "\n",
    "        for decoder_layer in self.layers[: self.config.num_hidden_layers]:\n",
    "            if output_hidden_states:\n",
    "                all_hidden_states += (hidden_states,)\n",
    "\n",
    "            if self.gradient_checkpointing and self.training:\n",
    "                layer_outputs = self._gradient_checkpointing_func(\n",
    "                    decoder_layer.__call__,\n",
    "                    hidden_states,\n",
    "                    causal_mask,\n",
    "                    position_ids,\n",
    "                    past_key_values,\n",
    "                    output_attentions,\n",
    "                    use_cache,\n",
    "                    cache_position,\n",
    "                    position_embeddings,\n",
    "                )\n",
    "            else:\n",
    "                layer_outputs = decoder_layer(\n",
    "                    hidden_states,\n",
    "                    attention_mask=causal_mask,\n",
    "                    position_ids=position_ids,\n",
    "                    past_key_value=past_key_values,\n",
    "                    output_attentions=output_attentions,\n",
    "                    use_cache=use_cache,\n",
    "                    cache_position=cache_position,\n",
    "                    position_embeddings=position_embeddings,\n",
    "                    **flash_attn_kwargs,\n",
    "                )\n",
    "\n",
    "            hidden_states = layer_outputs[0]\n",
    "\n",
    "            if output_attentions:\n",
    "                all_self_attns += (layer_outputs[1],)\n",
    "\n",
    "        hidden_states = self.norm(hidden_states)\n",
    "\n",
    "        # add hidden states from the last decoder layer\n",
    "        if output_hidden_states:\n",
    "            all_hidden_states += (hidden_states,)\n",
    "\n",
    "        output = BaseModelOutputWithPast(\n",
    "            last_hidden_state=hidden_states,\n",
    "            past_key_values=past_key_values if use_cache else None,\n",
    "            hidden_states=all_hidden_states,\n",
    "            attentions=all_self_attns,\n",
    "        )\n",
    "        return output if return_dict else output.to_tuple()\n",
    "\n",
    "@torch.compile(fullgraph = True, dynamic = True, options = torch_compile_options)\n",
    "def compiled_compute_loss(self, model, inputs, return_outputs=False, num_items_in_batch=None):\n",
    "\n",
    "    kwargs = {\n",
    "        \"input_ids\": inputs.data[\"input_ids\"],\n",
    "        \"attention_mask\": inputs.data[\"attention_mask\"],\n",
    "        \"labels\": inputs.data[\"labels\"],\n",
    "    }\n",
    "\n",
    "    outputs = model(**kwargs)\n",
    "    logits = outputs.logits  # shape: (B, S, V)\n",
    "    \n",
    "    # For causal language modeling, shift logits and labels so that\n",
    "    # prediction at time t is compared with label at time t+1.\n",
    "    shift_logits = logits[:, :-1, :]      # shape: (B, S-1, V)\n",
    "    shift_labels = inputs[\"labels\"][:, 1:]  # shape: (B, S-1)\n",
    "    \n",
    "    # Flatten the tensors for cross entropy: (B*(S-1), V) and (B*(S-1))\n",
    "    loss = torch.nn.functional.cross_entropy(\n",
    "        shift_logits.reshape(-1, shift_logits.size(-1)),\n",
    "        shift_labels.reshape(-1),\n",
    "        ignore_index=-100\n",
    "    )\n",
    "    return loss\n",
    "\n",
    "transformers.models.llama.modeling_llama.LlamaMLP.forward = compiled_llama_mlp\n",
    "transformers.models.llama.modeling_llama.LlamaRMSNorm.forward = compiled_llama_rms_norm\n",
    "transformers.models.llama.modeling_llama.LlamaRotaryEmbedding.forward = compiled_llama_rotary_embedding\n",
    "transformers.models.llama.modeling_llama.LlamaAttention.forward = compiled_llama_attention\n",
    "transformers.models.llama.modeling_llama.LlamaDecoderLayer.forward = compiled_llama_decoder_layer\n",
    "transformers.models.llama.modeling_llama.LlamaModel.forward = compiled_llama_model\n",
    "SFTTrainer.compute_loss = compiled_compute_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up LLaMA-3.2-1B with LoRA and 4-bit Quantization\n",
    "\n",
    "This script loads and optimizes the **LLaMA-3.2-1B-Instruct** model with **4-bit quantization** and **LoRA fine-tuning**.\n",
    "\n",
    "### Steps:\n",
    "1. **Set Precision & Load Model:**\n",
    "   - Uses `torch.float16` for efficiency.\n",
    "   - Loads a **4-bit quantized** model from `\"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"`.\n",
    "   - Utilizes `BitsAndBytesConfig` for efficient memory usage.\n",
    "\n",
    "2. **Tokenizer Setup:**\n",
    "   - Uses the `AutoTokenizer` with **right-side padding**.\n",
    "\n",
    "3. **LoRA Configuration:**\n",
    "   - Applies **Low-Rank Adaptation (LoRA)** with:\n",
    "     - `r = 32`\n",
    "     - `lora_alpha = 64`\n",
    "     - Applied to **attention and MLP layers**.\n",
    "\n",
    "4. **Enable Gradients for LoRA Parameters:**\n",
    "   - Only **LoRA parameters** are trainable.\n",
    "   - Freezes other model weights.\n",
    "\n",
    "5. **Enable Input Gradients:**\n",
    "   - Calls `model.enable_input_require_grads()` to support **gradient-based methods**.\n",
    "\n",
    "This setup is optimized for fine-tuning with low VRAM usage.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_seq_length = 1024\n",
    "torch.set_default_dtype(torch.float16)\n",
    "model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    "dtype = torch.float16\n",
    "\n",
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit              = True,\n",
    "    bnb_4bit_use_double_quant = True,\n",
    "    bnb_4bit_quant_type       = \"nf4\",\n",
    "    bnb_4bit_compute_dtype    = dtype,\n",
    ")\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    device_map = \"auto\",\n",
    "    attn_implementation = \"sdpa\",\n",
    "    # quantization_config = bnb_config,  ---> No need to pass as the model already has its own quantization config\n",
    ")\n",
    "\n",
    "model = model.to(\"cuda:0\")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "tokenizer.padding_side = \"right\"\n",
    "\n",
    "lora_config = LoraConfig(\n",
    "    r = 32,\n",
    "    lora_alpha = 64,\n",
    "    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n",
    "                      \"gate_proj\", \"up_proj\", \"down_proj\"],\n",
    "    lora_dropout = 0,\n",
    "    bias = \"none\",\n",
    "    task_type = TaskType.CAUSAL_LM,\n",
    ")\n",
    "\n",
    "# Get LoRA and setup model\n",
    "model = get_peft_model(model, lora_config)\n",
    "\n",
    "with torch.no_grad():\n",
    "    for name, param in model.named_parameters():\n",
    "        if \".lora_A.\" in name or \".lora_B.\" in name: param.requires_grad_(True)\n",
    "        else: param.requires_grad_(False)\n",
    "\n",
    "model.enable_input_require_grads()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Dataset  \n",
    "\n",
    "- Fetches the `unified_chip2.jsonl` dataset from Hugging Face.  \n",
    "- Loads it as a JSON dataset.  \n",
    "- Uses only the first 10% of the training split.  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset\n",
    "url = \"https://huggingface.co/datasets/laion/OIG/resolve/main/unified_chip2.jsonl\"\n",
    "dataset = load_dataset(\"json\", data_files = {\"train\" : url}, split = \"train[:20%]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Timing Callback for Separate Warmup and Main Training Runs\n",
    "In this cell, we define a custom TrainerCallback called `TimingCallback`.  \n",
    "- **Warmup Steps:** The first three steps (where recompilation occurs) are timed separately.  \n",
    "- **Main Training Steps:** The remaining steps are timed separately.  \n",
    "\n",
    "When added to SFTTrainer, this callback will print the time for each step and summarize the total times at the end of training.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from transformers import TrainerCallback, TrainingArguments, TrainerState, TrainerControl\n",
    "\n",
    "class TimingCallback(TrainerCallback):\n",
    "    def __init__(self, warmup_steps: int = 3):\n",
    "        self.warmup_steps = warmup_steps\n",
    "        self.warmup_time = 0.0\n",
    "        self.main_time = 0.0\n",
    "\n",
    "    def on_step_begin(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        # Record the start time of the step.\n",
    "        self.step_start = time.time()\n",
    "\n",
    "    def on_step_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        step_time = time.time() - self.step_start\n",
    "        # If the current step is within the warmup steps, add time to warmup_time.\n",
    "        if state.global_step <= self.warmup_steps:\n",
    "            self.warmup_time += step_time\n",
    "        else:\n",
    "            self.main_time += step_time\n",
    "\n",
    "    def on_train_end(self, args: TrainingArguments, state: TrainerState, control: TrainerControl, **kwargs):\n",
    "        total_steps = state.global_step\n",
    "        main_steps = total_steps - self.warmup_steps\n",
    "        print(f\"\\nTotal warmup time for {self.warmup_steps} steps: {self.warmup_time:.4f} seconds\")\n",
    "        print(f\"Total main training time for {main_steps} steps: {self.main_time:.4f} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Configuration  \n",
    "\n",
    "- Uses `SFTTrainer` to fine-tune the model.  \n",
    "- Loads dataset and tokenizer for training.  \n",
    "- Training settings:  \n",
    "  - Batch size: 1  \n",
    "  - Gradient accumulation: 2 steps  \n",
    "  - Warmup: 3 steps  \n",
    "  - Max steps: 100  \n",
    "  - Logs every step  \n",
    "  - Outputs saved to `\"outputs\"`  \n",
    "  - Uses FP16 or BF16 based on model dtype  \n",
    "- Starts training with `.train()`  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/unsloth_3/lib/python3.12/site-packages/torch/_inductor/compile_fx.py:239: UserWarning: TensorFloat32 tensor cores for float32 matrix multiplication available but not enabled. Consider setting `torch.set_float32_matmul_precision('high')` for better performance.\n",
      "  warnings.warn(\n",
      "W0305 21:05:31.131000 432 torch/_inductor/utils.py:1213] [0/0] Not enough SMs to use max_autotune_gemm mode\n",
      "W0305 21:06:45.585000 432 torch/_inductor/debug.py:454] [0/0] model__0_forward_1 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__0_forward_1.0\n",
      "W0305 21:07:36.334000 432 torch/_inductor/debug.py:454] [0/0] model__0_backward_2 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__0_backward_2.1\n",
      "V0305 21:07:46.065000 432 torch/_dynamo/guards.py:2974] [0/1] [__recompiles_verbose] Recompiling function compiled_compute_loss in /tmp/ipykernel_432/307889903.py:232\n",
      "V0305 21:07:46.065000 432 torch/_dynamo/guards.py:2974] [0/1] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
      "V0305 21:07:46.065000 432 torch/_dynamo/guards.py:2974] [0/1] [__recompiles_verbose]     guard 0 failures:\n",
      "V0305 21:07:46.065000 432 torch/_dynamo/guards.py:2974] [0/1] [__recompiles_verbose]     - 0/0: ___check_obj_id(L['model']._modules['base_model']._modules['model']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 10654592)\n",
      "W0305 21:09:36.181000 432 torch/_inductor/debug.py:454] [0/1] model__1_forward_4 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__1_forward_4.2\n",
      "W0305 21:10:27.277000 432 torch/_inductor/debug.py:454] [0/1] model__1_backward_5 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__1_backward_5.3\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [150/150 05:56, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>3.655900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>5.998000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.349700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>5.238400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.405300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.698200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.625100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.355800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.245100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.787100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>4.813300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>5.323600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.480600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.935300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.970500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.319300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>4.727500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>5.638500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>2.951100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>6.600100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>21</td>\n",
       "      <td>4.697100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>22</td>\n",
       "      <td>2.874700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>23</td>\n",
       "      <td>3.148500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>24</td>\n",
       "      <td>3.627700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>25</td>\n",
       "      <td>3.144800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>26</td>\n",
       "      <td>6.847900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>27</td>\n",
       "      <td>7.648500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>28</td>\n",
       "      <td>2.798900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>29</td>\n",
       "      <td>5.828900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.883000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>31</td>\n",
       "      <td>3.205400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>32</td>\n",
       "      <td>5.034300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>33</td>\n",
       "      <td>5.836800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>34</td>\n",
       "      <td>5.353800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>35</td>\n",
       "      <td>5.086500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>36</td>\n",
       "      <td>4.831900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>37</td>\n",
       "      <td>6.339200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>38</td>\n",
       "      <td>4.563200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>39</td>\n",
       "      <td>3.298300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>4.750800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>41</td>\n",
       "      <td>4.949200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>42</td>\n",
       "      <td>3.419100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>43</td>\n",
       "      <td>4.922300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>44</td>\n",
       "      <td>3.419300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>45</td>\n",
       "      <td>2.757100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>46</td>\n",
       "      <td>2.999300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>47</td>\n",
       "      <td>5.291500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>48</td>\n",
       "      <td>3.979400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>49</td>\n",
       "      <td>4.318100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>8.053000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>51</td>\n",
       "      <td>3.815000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>52</td>\n",
       "      <td>2.771600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>53</td>\n",
       "      <td>5.858000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>54</td>\n",
       "      <td>2.966400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>55</td>\n",
       "      <td>3.268400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>56</td>\n",
       "      <td>2.343500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>57</td>\n",
       "      <td>2.852600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>58</td>\n",
       "      <td>2.720400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>59</td>\n",
       "      <td>3.471100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>2.557100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>61</td>\n",
       "      <td>2.734200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>62</td>\n",
       "      <td>2.827100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>63</td>\n",
       "      <td>5.036000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>64</td>\n",
       "      <td>5.623200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>65</td>\n",
       "      <td>3.316600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>66</td>\n",
       "      <td>3.562800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>67</td>\n",
       "      <td>2.991400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>68</td>\n",
       "      <td>3.031700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>69</td>\n",
       "      <td>2.885200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>3.221000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>71</td>\n",
       "      <td>3.463600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>72</td>\n",
       "      <td>3.230300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>73</td>\n",
       "      <td>3.023900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>74</td>\n",
       "      <td>2.983800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>75</td>\n",
       "      <td>2.893500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>76</td>\n",
       "      <td>4.381000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>77</td>\n",
       "      <td>3.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>78</td>\n",
       "      <td>3.183400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>79</td>\n",
       "      <td>3.439900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>3.076100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>81</td>\n",
       "      <td>4.660000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>82</td>\n",
       "      <td>3.419600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>83</td>\n",
       "      <td>4.344500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>84</td>\n",
       "      <td>1.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>85</td>\n",
       "      <td>2.849600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>86</td>\n",
       "      <td>2.605200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>87</td>\n",
       "      <td>3.939600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>88</td>\n",
       "      <td>4.088900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>89</td>\n",
       "      <td>3.300800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>3.202300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>91</td>\n",
       "      <td>3.736200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>92</td>\n",
       "      <td>4.491600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>93</td>\n",
       "      <td>3.718200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>94</td>\n",
       "      <td>3.100500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>95</td>\n",
       "      <td>2.527600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>96</td>\n",
       "      <td>5.326100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>97</td>\n",
       "      <td>2.570200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>98</td>\n",
       "      <td>2.878100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>99</td>\n",
       "      <td>4.343600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>4.030100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>101</td>\n",
       "      <td>3.009200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>102</td>\n",
       "      <td>2.273100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>103</td>\n",
       "      <td>3.351400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>104</td>\n",
       "      <td>3.453100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>105</td>\n",
       "      <td>4.225700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>106</td>\n",
       "      <td>2.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>107</td>\n",
       "      <td>3.900500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>108</td>\n",
       "      <td>3.199600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>109</td>\n",
       "      <td>2.911700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>2.960300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>111</td>\n",
       "      <td>3.223800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>112</td>\n",
       "      <td>2.650300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>113</td>\n",
       "      <td>2.892700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>114</td>\n",
       "      <td>3.565900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>115</td>\n",
       "      <td>2.893400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>116</td>\n",
       "      <td>2.645200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>117</td>\n",
       "      <td>3.510700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>118</td>\n",
       "      <td>2.609400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>119</td>\n",
       "      <td>5.120300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>2.669400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>121</td>\n",
       "      <td>3.266800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>122</td>\n",
       "      <td>3.755700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>123</td>\n",
       "      <td>4.191400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>124</td>\n",
       "      <td>4.437400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>125</td>\n",
       "      <td>5.056300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>126</td>\n",
       "      <td>3.754500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>127</td>\n",
       "      <td>3.425700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>128</td>\n",
       "      <td>2.733000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>129</td>\n",
       "      <td>4.340800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>3.648400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>131</td>\n",
       "      <td>3.214600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>132</td>\n",
       "      <td>4.313900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>133</td>\n",
       "      <td>2.655000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>134</td>\n",
       "      <td>6.511600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>135</td>\n",
       "      <td>5.449700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>136</td>\n",
       "      <td>5.176700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>137</td>\n",
       "      <td>2.584900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>138</td>\n",
       "      <td>2.947300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>139</td>\n",
       "      <td>5.131900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>3.209900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>141</td>\n",
       "      <td>3.785600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>142</td>\n",
       "      <td>3.377500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>143</td>\n",
       "      <td>3.406800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>144</td>\n",
       "      <td>5.073600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>145</td>\n",
       "      <td>4.750300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>146</td>\n",
       "      <td>2.430000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>147</td>\n",
       "      <td>2.970100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>148</td>\n",
       "      <td>2.774400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>149</td>\n",
       "      <td>5.971800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>6.821900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "V0305 21:10:34.343000 432 torch/_dynamo/guards.py:2974] [0/2] [__recompiles_verbose] Recompiling function compiled_compute_loss in /tmp/ipykernel_432/307889903.py:232\n",
      "V0305 21:10:34.343000 432 torch/_dynamo/guards.py:2974] [0/2] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
      "V0305 21:10:34.343000 432 torch/_dynamo/guards.py:2974] [0/2] [__recompiles_verbose]     guard 0 failures:\n",
      "V0305 21:10:34.343000 432 torch/_dynamo/guards.py:2974] [0/2] [__recompiles_verbose]     - 0/1: ((L['inputs'].data['input_ids'].size()[1]*L['inputs'].data['input_ids'].size()[1]) % 8) != 0  # attn_output = torch.nn.functional.scaled_dot_product_attention(  # transformers/integrations/sdpa_attention.py:53 in sdpa_attention_forward (_dynamo/utils.py:3197 in run_node)\n",
      "V0305 21:10:34.343000 432 torch/_dynamo/guards.py:2974] [0/2] [__recompiles_verbose] \n",
      "V0305 21:10:34.343000 432 torch/_dynamo/guards.py:2974] [0/2] [__recompiles_verbose]     guard 1 failures:\n",
      "V0305 21:10:34.343000 432 torch/_dynamo/guards.py:2974] [0/2] [__recompiles_verbose]     - 0/0: ___check_obj_id(L['model']._modules['base_model']._modules['model']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 10654592)\n",
      "W0305 21:12:25.388000 432 torch/_inductor/debug.py:454] [0/2] model__2_forward_7 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__2_forward_7.4\n",
      "W0305 21:13:10.937000 432 torch/_inductor/debug.py:454] [0/2] model__2_backward_8 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__2_backward_8.5\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose] Recompiling function compiled_compute_loss in /tmp/ipykernel_432/307889903.py:232\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose]     triggered by the following guard failure(s):\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose]     guard 0 failures:\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose]     - 0/1: ((L['inputs'].data['input_ids'].size()[1]*L['inputs'].data['input_ids'].size()[1]) % 8) != 0  # attn_output = torch.nn.functional.scaled_dot_product_attention(  # transformers/integrations/sdpa_attention.py:53 in sdpa_attention_forward (_dynamo/utils.py:3197 in run_node)\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose] \n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose]     guard 1 failures:\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose]     - 0/2: (L['inputs'].data['input_ids'].size()[1] % 8) != 0  # attn_output = torch.nn.functional.scaled_dot_product_attention(  # transformers/integrations/sdpa_attention.py:53 in sdpa_attention_forward (_dynamo/utils.py:3197 in run_node)\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose] \n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose]     guard 2 failures:\n",
      "V0305 21:13:15.118000 432 torch/_dynamo/guards.py:2974] [0/3] [__recompiles_verbose]     - 0/0: ___check_obj_id(L['model']._modules['base_model']._modules['model']._modules['model']._modules['layers']._modules['0']._modules['mlp']._modules['up_proj']._modules['base_layer'].compute_type_is_set, 10654592)\n",
      "W0305 21:14:59.404000 432 torch/_inductor/debug.py:454] [0/3] model__3_forward_10 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__3_forward_10.6\n",
      "W0305 21:15:41.322000 432 torch/_inductor/debug.py:454] [0/3] model__3_backward_11 debug trace: /mnt/c/Users/FAHAD/Downloads/unsloth/unsloth3/torch_compile_debug/run_2025_03_05_21_05_22_665777-pid_432/torchinductor/model__3_backward_11.7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total warmup time for 5 steps: 684.2920 seconds\n",
      "Total main training time for 145 steps: 40.7494 seconds\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=150, training_loss=3.8655019982655845, metrics={'train_runtime': 729.3873, 'train_samples_per_second': 0.411, 'train_steps_per_second': 0.206, 'total_flos': 159975600611328.0, 'train_loss': 3.8655019982655845})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    train_dataset = dataset,\n",
    "    processing_class = tokenizer,\n",
    "    args = SFTConfig(\n",
    "        per_device_train_batch_size = 1,\n",
    "        gradient_accumulation_steps = 2,\n",
    "        warmup_steps = 5,\n",
    "        max_steps = 150,\n",
    "        logging_steps = 1,\n",
    "        output_dir = \"outputs\",\n",
    "        seed = 3407,\n",
    "        max_seq_length = max_seq_length,\n",
    "        fp16 = model.get_input_embeddings().weight.dtype == torch.float16,\n",
    "        bf16 = model.get_input_embeddings().weight.dtype == torch.bfloat16,\n",
    "        report_to = \"none\", # For W&B\n",
    "        dataset_num_proc = 4,\n",
    "        label_names = [\"input_ids\", \"labels\", \"attention_mask\"]\n",
    "    ),\n",
    "    callbacks = [TimingCallback(warmup_steps=5)]\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
